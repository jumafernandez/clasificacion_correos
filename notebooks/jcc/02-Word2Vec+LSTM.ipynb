{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "02-Word2Vec+LSTM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumafernandez/clasificacion_correos/blob/main/notebooks/jcc/02-Word2Vec%2BLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPlN-I2S8h5a"
      },
      "source": [
        "# LSTM+Word2Vec\n",
        "\n",
        "En esta notebook, se entrena y prueba la clasificación de oraciones usando LSTM y Word2Vec pre-entrenado.\n",
        "\n",
        "El principal beneficio de la incrustación de palabras es que incluso las palabras que no se ven durante el entrenamiento se pueden predecir bien ya que la incrustación de palabras está pre-entrenada con un conjunto de datos más grande que los del dataset actual.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEytdZIxFEhM"
      },
      "source": [
        "## Carga de librerías, modelo word2vec pre-entrenado y funciones útiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJmDIfYu8h5g"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "VECTOR_EMBEDDINGS = 250"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKOBOXgA8h5g"
      },
      "source": [
        "# Load Pretrained Word2Vec\n",
        "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkZ3Ho5JGInp",
        "outputId": "a4cc44f6-5c74-4fd8-876d-2d486c41c647"
      },
      "source": [
        "embed(['Juan', 'Fernandwxxdo', 'Mujer'])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 1006 calls to <function recreate_function.<locals>.restored_function_body at 0x7fc6529eec20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 1006 calls to <function recreate_function.<locals>.restored_function_body at 0x7fc6529eec20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 250), dtype=float32, numpy=\n",
              "array([[ 1.42693724e-02, -9.08979494e-03, -3.98036987e-02,\n",
              "         3.62966694e-02,  9.49378684e-02,  1.33175060e-01,\n",
              "         5.39111458e-02,  1.10701146e-02,  1.46255925e-01,\n",
              "        -2.13303957e-02, -3.83548699e-02,  4.41266783e-02,\n",
              "        -9.04915407e-02,  2.29147449e-02, -3.18627581e-02,\n",
              "        -1.03184409e-01, -1.68420419e-01, -4.47667427e-02,\n",
              "         2.33685635e-02, -5.40872291e-03,  1.03382301e-03,\n",
              "        -1.41673787e-02,  5.66975288e-02, -1.82882976e-02,\n",
              "        -1.00069664e-01, -1.74700860e-02,  9.95630622e-02,\n",
              "         9.80864614e-02, -6.69433102e-02,  4.79473807e-02,\n",
              "         4.57753688e-02, -2.03111470e-02, -5.50246350e-02,\n",
              "        -1.25571623e-01, -2.35121641e-02, -7.92662799e-02,\n",
              "        -5.50914090e-03, -7.49683520e-03,  1.01595342e-01,\n",
              "        -2.06922018e-03,  1.18362702e-01,  1.83063328e-01,\n",
              "         6.25873879e-02, -1.19993433e-01,  5.89830708e-03,\n",
              "        -6.69010654e-02, -6.98054358e-02,  5.13526574e-02,\n",
              "         9.86785218e-02, -4.74440344e-02, -3.79690602e-02,\n",
              "         2.68850457e-02, -5.42480648e-02,  2.32462566e-02,\n",
              "        -3.45015638e-02, -5.23175225e-02,  4.65277508e-02,\n",
              "        -7.19419345e-02, -5.87158911e-02,  5.19745499e-02,\n",
              "         2.98365690e-02,  1.93497594e-02, -1.75908618e-02,\n",
              "        -2.76173488e-03,  9.21678320e-02, -1.85800511e-02,\n",
              "        -7.30047701e-03, -1.77053120e-02,  1.90829355e-02,\n",
              "         7.90467858e-02, -1.72469544e-03,  1.35697080e-02,\n",
              "         8.94216374e-02,  2.04151068e-02, -1.64803735e-03,\n",
              "         6.26052022e-02,  1.72572918e-02,  2.54255347e-02,\n",
              "         2.56768931e-02,  1.25367448e-01,  3.71184312e-02,\n",
              "        -1.37668904e-02,  4.46468070e-02, -7.12112039e-02,\n",
              "         3.79162561e-03,  8.76099989e-02, -9.22193937e-03,\n",
              "        -7.57505000e-02, -1.85017549e-02, -1.35670286e-02,\n",
              "        -1.03130013e-01,  6.50389493e-02, -7.67223611e-02,\n",
              "         4.39921953e-02,  1.24096535e-01, -2.70337835e-02,\n",
              "        -3.57073918e-02,  7.61302887e-03, -1.27806261e-01,\n",
              "         2.68262345e-02, -1.01840138e-01,  1.64148714e-02,\n",
              "        -1.38659524e-02, -1.51718715e-02,  1.40328435e-02,\n",
              "        -2.85456073e-03, -1.33035993e-02, -6.29276633e-02,\n",
              "        -1.17503712e-02, -2.30422392e-02, -1.82621591e-02,\n",
              "         9.49834883e-02,  3.49014327e-02, -8.54882374e-02,\n",
              "         1.33369304e-02,  4.32894146e-03, -7.44165778e-02,\n",
              "        -4.11955975e-02,  6.19838424e-02,  7.83430636e-02,\n",
              "        -8.21765047e-03, -5.96240722e-02,  3.83033454e-02,\n",
              "        -3.57661247e-02,  2.79451925e-02,  8.10119063e-02,\n",
              "        -1.51990846e-01,  1.98047925e-02, -1.59424782e-01,\n",
              "        -2.96888780e-02,  7.72909969e-02,  1.70794223e-02,\n",
              "        -1.15334718e-02, -4.71375845e-02,  2.62148362e-02,\n",
              "        -1.01230701e-03, -4.75559086e-02, -1.31810317e-02,\n",
              "         4.95224493e-03,  6.37418330e-02,  6.20182045e-02,\n",
              "        -7.68911513e-03,  2.03353222e-02, -1.68953892e-02,\n",
              "         1.50944352e-01, -5.79893999e-02, -4.12820950e-02,\n",
              "         8.81798193e-02, -1.88302137e-02,  1.02596112e-01,\n",
              "         1.84196949e-01, -2.90659536e-02, -5.98726124e-02,\n",
              "        -3.90955396e-02,  2.23669154e-03, -8.53595790e-03,\n",
              "        -6.38910756e-02,  7.32716992e-02,  1.79242361e-02,\n",
              "         2.88203396e-02,  7.06118122e-02,  2.50109453e-02,\n",
              "        -6.05650768e-02, -1.72991734e-02,  7.04014376e-02,\n",
              "        -2.17808853e-03,  3.62904854e-02,  1.04317851e-02,\n",
              "        -7.36337900e-03, -6.22689091e-02, -4.08128724e-02,\n",
              "         9.36285183e-02, -4.39216308e-02, -1.23507157e-02,\n",
              "        -6.21290691e-02, -1.03764217e-02, -1.15472071e-01,\n",
              "        -9.05874595e-02,  3.25069204e-02, -7.04135820e-02,\n",
              "         6.83954209e-02, -8.34057033e-02, -6.36068657e-02,\n",
              "         1.10833883e-01, -7.09087625e-02,  5.07912636e-02,\n",
              "        -3.36274542e-02,  4.28722091e-02, -6.86583072e-02,\n",
              "         1.71605982e-02, -3.48829627e-02, -5.35572655e-02,\n",
              "         8.15178640e-03,  9.36038233e-03,  1.12928962e-02,\n",
              "         8.26448575e-02, -5.97820878e-02,  1.82693228e-01,\n",
              "        -1.20830927e-02,  5.29609434e-02, -1.91373155e-02,\n",
              "         1.05912566e-01, -1.76809393e-02, -3.98409888e-02,\n",
              "        -1.43806245e-02, -9.41946208e-02,  1.28530398e-01,\n",
              "        -4.19030078e-02,  8.88157636e-02,  6.32923171e-02,\n",
              "        -4.08117240e-03, -4.36215550e-02,  2.47506294e-02,\n",
              "        -1.66378934e-02, -2.78592650e-02, -1.90938227e-02,\n",
              "         3.22288722e-02, -1.72371417e-03, -3.46815921e-02,\n",
              "         3.54394130e-02, -1.56898256e-02, -1.56328455e-02,\n",
              "         3.32239107e-03, -1.52001372e-02,  7.66660199e-02,\n",
              "        -1.28669515e-01,  6.70695603e-02,  2.09314246e-02,\n",
              "        -3.83132249e-02,  7.26406276e-02,  6.32070750e-03,\n",
              "        -3.23767252e-02, -8.72925892e-02, -5.77796958e-02,\n",
              "         2.37928145e-02, -2.81831156e-02,  7.95223564e-02,\n",
              "         3.60008292e-02,  1.00011133e-01,  4.94831912e-02,\n",
              "        -1.60294965e-01, -4.57065552e-02,  2.50317715e-02,\n",
              "         3.21962982e-02, -4.00562882e-02,  6.75398111e-02,\n",
              "         5.96447513e-02, -8.47473294e-02, -7.45044947e-02,\n",
              "         1.03357118e-02],\n",
              "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00],\n",
              "       [ 5.05077615e-02, -4.57254238e-02,  8.35906249e-03,\n",
              "         1.40917674e-02,  8.10062587e-02,  1.38562128e-01,\n",
              "         3.97385135e-02, -4.88677397e-02, -1.23672159e-02,\n",
              "        -5.65481000e-02,  5.86169213e-03,  4.36024442e-02,\n",
              "        -2.82429159e-02, -2.65751667e-02, -6.87389448e-02,\n",
              "        -3.88379991e-02, -1.06135152e-01, -7.11640995e-03,\n",
              "         6.02871925e-02, -9.81157348e-02,  1.07620358e-01,\n",
              "         4.84369666e-05, -1.80547684e-02, -1.15533575e-01,\n",
              "        -1.29158393e-01, -2.32522525e-02,  4.84780855e-02,\n",
              "         1.01936460e-02, -9.25440490e-02,  4.56210040e-03,\n",
              "        -3.93654406e-02, -1.90909859e-02, -7.26842806e-02,\n",
              "        -1.11198880e-01, -4.38087434e-02, -2.17078216e-02,\n",
              "        -7.89933279e-02, -4.96781021e-02,  4.03942429e-02,\n",
              "        -8.94450955e-03,  9.08657163e-02,  1.06025472e-01,\n",
              "         3.77326459e-02, -4.75986637e-02, -1.31135747e-01,\n",
              "        -1.05339587e-01, -6.85628504e-02,  2.43450664e-02,\n",
              "         5.69506139e-02, -5.95110245e-02, -2.65809614e-02,\n",
              "        -4.04415242e-02, -6.55275807e-02, -2.02486780e-03,\n",
              "         3.83186489e-02,  2.59554014e-03,  2.09952760e-02,\n",
              "        -4.03130129e-02, -3.88275236e-02,  2.42263786e-02,\n",
              "        -5.51076140e-03, -2.74470672e-02,  7.88258016e-03,\n",
              "        -5.89058781e-03,  1.42843621e-02, -2.72204634e-02,\n",
              "        -3.43357995e-02,  7.63311014e-02,  3.55603546e-02,\n",
              "         6.87380806e-02,  1.94031689e-02,  1.70530565e-02,\n",
              "         5.38502969e-02,  3.19437496e-02,  1.53390085e-02,\n",
              "         7.07568750e-02, -1.98580921e-02, -1.91377141e-02,\n",
              "         7.35514611e-02,  4.24135514e-02,  1.82028878e-02,\n",
              "         4.46820408e-02, -3.58651206e-02,  2.52837259e-02,\n",
              "        -1.37976091e-02,  2.73831319e-02, -6.52695298e-02,\n",
              "        -1.32525396e-02,  1.30588580e-02,  2.22123004e-02,\n",
              "        -4.90368493e-02,  2.64337510e-02, -3.65522392e-02,\n",
              "         1.46555603e-01,  7.61619881e-02, -8.67860690e-02,\n",
              "        -1.00541219e-01,  1.90170631e-02, -3.18718404e-02,\n",
              "        -7.52478614e-02, -6.64475933e-02,  7.62128308e-02,\n",
              "        -2.40636934e-02, -5.44913597e-02,  1.20145641e-01,\n",
              "         7.36167505e-02, -2.56284215e-02,  8.09508413e-02,\n",
              "        -2.37965994e-02, -5.37640080e-02, -2.98645943e-02,\n",
              "        -4.90740947e-02,  3.16124260e-02, -1.91861451e-01,\n",
              "         3.97407487e-02,  3.75036639e-03, -9.50634032e-02,\n",
              "         6.26127645e-02,  5.23369648e-02,  8.78381580e-02,\n",
              "         2.02214830e-02, -6.61408752e-02,  7.18608946e-02,\n",
              "         1.88872423e-02, -6.31456971e-02,  1.38155445e-02,\n",
              "        -5.93778417e-02,  5.38386926e-02, -1.34110510e-01,\n",
              "         1.35197565e-02,  9.24942866e-02, -6.67500868e-02,\n",
              "        -5.15106916e-02, -1.68045253e-01, -5.31047247e-02,\n",
              "         4.33648750e-03, -1.40855327e-01,  6.81505911e-03,\n",
              "         4.84212637e-02, -1.67215187e-02, -3.00840344e-02,\n",
              "        -7.05541223e-02,  4.22935933e-02, -7.86039382e-02,\n",
              "         4.58832420e-02, -8.45680982e-02, -9.70954001e-02,\n",
              "        -4.60723452e-02, -4.41222303e-02, -1.10599743e-02,\n",
              "         3.99131291e-02, -1.15931116e-01,  5.74806109e-02,\n",
              "         5.63348904e-02, -6.22817948e-02, -2.98523139e-02,\n",
              "        -7.28857666e-02,  6.97793439e-02, -5.67431413e-02,\n",
              "        -1.51104126e-02,  1.41015137e-02, -5.84255643e-02,\n",
              "         6.67654327e-04, -3.49716470e-02,  5.14070969e-03,\n",
              "        -1.17155090e-02, -5.00313975e-02, -4.10583541e-02,\n",
              "         2.55726520e-02, -1.25076607e-01, -4.29669172e-02,\n",
              "         7.89067298e-02, -3.34990695e-02, -2.89120972e-02,\n",
              "        -1.19951919e-01, -1.45081133e-01, -7.81301707e-02,\n",
              "         5.22778332e-02,  8.45442340e-03,  6.34331070e-03,\n",
              "         3.79544497e-02, -3.67011279e-02, -1.87372826e-02,\n",
              "         3.08460221e-02, -7.51449987e-02,  9.42666754e-02,\n",
              "         1.11715926e-03,  1.88040491e-02, -8.17061961e-03,\n",
              "        -6.14056811e-02, -4.02443558e-02, -1.48670793e-01,\n",
              "        -1.13925543e-02, -9.04699117e-02, -8.06211121e-03,\n",
              "         1.10406727e-02, -1.42961098e-02,  1.05041690e-01,\n",
              "         2.17560306e-02,  3.72949503e-02, -7.83508420e-02,\n",
              "         7.46745020e-02, -2.08816230e-02,  3.57091660e-03,\n",
              "        -9.31990892e-02,  6.56412542e-02,  7.39742592e-02,\n",
              "         3.13347913e-02, -4.38256636e-02,  1.01681769e-01,\n",
              "        -5.94522245e-02, -1.25927269e-01, -3.67262140e-02,\n",
              "         6.38400689e-02, -2.56548058e-02,  3.11628712e-04,\n",
              "        -6.55512586e-02,  5.93135990e-02, -8.15717801e-02,\n",
              "         7.77260773e-03, -8.06079060e-02,  9.70535562e-04,\n",
              "        -9.16384012e-02,  1.04796570e-02,  6.85588792e-02,\n",
              "        -1.15260236e-01,  9.53003615e-02, -6.47642836e-02,\n",
              "        -1.65793356e-02, -2.23728530e-02,  5.25435954e-02,\n",
              "        -7.79359415e-02, -4.88292947e-02, -6.25707433e-02,\n",
              "        -2.57678144e-02, -6.20754212e-02,  8.21120441e-02,\n",
              "        -2.00936869e-02,  9.46366340e-02,  9.49413553e-02,\n",
              "        -1.21169917e-01, -7.89288953e-02, -8.09638575e-02,\n",
              "         9.95671004e-02,  3.71442102e-02, -1.29333530e-02,\n",
              "         2.32433844e-02, -6.48271888e-02, -5.31046418e-03,\n",
              "         6.26051947e-02]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LiWpDyvI9L7"
      },
      "source": [
        "### Carga de Word2Vec en español"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDI1cDRNHzGq",
        "outputId": "66b83500-8456-4ea3-c259-33a9d144d409"
      },
      "source": [
        "# Cargo el Word2Vec pre-entrenado\r\n",
        "# Referencias: https://github.com/dccuchile/spanish-word-embeddings\r\n",
        "from os import path\r\n",
        "\r\n",
        "# Lo descargo desde la URL\r\n",
        "filename=\"SBW-vectors-300-min5.bin.gz\"\r\n",
        "if not(path.exists(filename)):\r\n",
        "  !wget http://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz\r\n",
        "\r\n",
        "# Lo cargo en la variable embeddings\r\n",
        "import gensim\r\n",
        "from gensim.models import Word2Vec\r\n",
        "\r\n",
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)\r\n",
        "embeddings.init_sims(replace=True)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-11 02:30:46--  http://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz\n",
            "Resolving cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)... 200.16.17.55\n",
            "Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz [following]\n",
            "--2021-03-11 02:30:47--  https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz\n",
            "Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1123304474 (1.0G) [application/x-gzip]\n",
            "Saving to: ‘SBW-vectors-300-min5.bin.gz’\n",
            "\n",
            "SBW-vectors-300-min 100%[===================>]   1.05G  17.6MB/s    in 62s     \n",
            "\n",
            "2021-03-11 02:31:50 (17.2 MB/s) - ‘SBW-vectors-300-min5.bin.gz’ saved [1123304474/1123304474]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa85mDxHFQIw"
      },
      "source": [
        "def get_max_length(text):\r\n",
        "    \"\"\"\r\n",
        "    get max token counts from train data, \r\n",
        "    so we use this number as fixed length input to RNN cell\r\n",
        "    \"\"\"\r\n",
        "    max_length = 0\r\n",
        "    for row in text:\r\n",
        "        if len(row.split(\" \")) > max_length:\r\n",
        "            max_length = len(row.split(\" \"))\r\n",
        "    return max_length\r\n",
        "\r\n",
        "def get_word2vec_enc(texts):\r\n",
        "    \"\"\"\r\n",
        "    get word2vec value for each word in sentence.\r\n",
        "    concatenate word in numpy array, so we can use it as RNN input\r\n",
        "    \"\"\"\r\n",
        "    encoded_texts = []\r\n",
        "    for text in texts:\r\n",
        "        tokens = text.split(\" \")\r\n",
        "        word2vec_embedding = embed(tokens)\r\n",
        "        encoded_texts.append(word2vec_embedding)\r\n",
        "    return encoded_texts\r\n",
        "        \r\n",
        "def get_padded_encoded_text(encoded_text, max_length):\r\n",
        "    \"\"\"\r\n",
        "    for short sentences, we prepend zero padding so all input to RNN has same length\r\n",
        "    \"\"\"\r\n",
        "    padded_text_encoding = []\r\n",
        "    for enc_text in encoded_text:\r\n",
        "        zero_padding_cnt = max_length - enc_text.shape[0]\r\n",
        "        pad = np.zeros((1, VECTOR_EMBEDDINGS))\r\n",
        "        for i in range(zero_padding_cnt):\r\n",
        "            enc_text = np.concatenate((pad, enc_text), axis=0)\r\n",
        "        padded_text_encoding.append(enc_text)\r\n",
        "    return padded_text_encoding\r\n",
        "\r\n",
        "def category_encode(category):\r\n",
        "    \"\"\"\r\n",
        "    Se encodea la clase en variables dummies\r\n",
        "    \"\"\"\r\n",
        "    return pd.get_dummies(category)\r\n",
        "\r\n",
        "\r\n",
        "def preprocess(x, y, max_length):\r\n",
        "    \"\"\"\r\n",
        "    encode text value to numeric value\r\n",
        "    \"\"\"\r\n",
        "    # encode words into word2vec\r\n",
        "    text = x.tolist()\r\n",
        "    \r\n",
        "    encoded_text = get_word2vec_enc(text)\r\n",
        "    padded_encoded_text = get_padded_encoded_text(encoded_text, max_length)\r\n",
        "    \r\n",
        "    # encoded class\r\n",
        "    categorys = y.tolist()\r\n",
        "    encoded_category = category_encode(categorys)\r\n",
        "    X = np.array(padded_encoded_text)\r\n",
        "    Y = np.array(encoded_category)\r\n",
        "    return X, Y "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs_qgHr0FRBm"
      },
      "source": [
        "## Carga del dataset y balanceo de clases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mamfYGa5CGBD"
      },
      "source": [
        "# Cargamos el archivo con las consultas que está en Github\r\n",
        "from os import path\r\n",
        "\r\n",
        "# En caso que no esté el archivo en Colab lo traigo\r\n",
        "if not(path.exists('03-Correos_variables_estaticas.csv')):\r\n",
        "  !wget https://raw.githubusercontent.com/jumafernandez/clasificacion_correos/main/data/03-Correos_variables_estaticas.csv\r\n",
        "\r\n",
        "# Leemos el archivo en un dataframe\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# Cargamos los datos\r\n",
        "df = pd.read_csv('03-Correos_variables_estaticas.csv', delimiter=\"|\")\r\n",
        "\r\n",
        "# Seleccionamos solo la consulta y la clase\r\n",
        "x_df = df[\"Consulta\"]\r\n",
        "y_df = df[\"Clase\"]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhbZo4YtCRzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daee09ea-c26f-4356-bfc8-1b1331768729"
      },
      "source": [
        "# Definición de la cantidad de clases (el resto se agrupa en OTRAS CONSULTAS)\r\n",
        "CANTIDAD_CLASES = 4\r\n",
        "\r\n",
        "# Transformamos todas las Clases minoritarias (Puedo ir variando la cantidad de clases que derivo a la Clase \"Otras Consultas\")\r\n",
        "clases = y_df.value_counts()\r\n",
        "clases_minoritarias = clases.iloc[CANTIDAD_CLASES-1:].keys().to_list()\r\n",
        "y_df.loc[y_df.isin(clases_minoritarias)] = \"Otras Consultas\"\r\n",
        "\r\n",
        "# Se numeriza la clase\r\n",
        "from sklearn import preprocessing\r\n",
        "le = preprocessing.LabelEncoder()\r\n",
        "y=le.fit_transform(y_df)\r\n",
        "\r\n",
        "# Me guardo las etiquetas de las clases (numerizadas)\r\n",
        "class_list=le.classes_\r\n",
        "\r\n",
        "class_list"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Boleto Universitario', 'Ingreso a la Universidad',\n",
              "       'Otras Consultas', 'Requisitos de Ingreso'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT1B_JENFXyV"
      },
      "source": [
        "## Separación en train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-d4Yf_DMFF"
      },
      "source": [
        "x = x_df\r\n",
        "y = y\r\n",
        "\r\n",
        "# Separo datos de entrenamiento y testing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# Separo en 80-20 entrenamiento/validación y testeo\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, test_size=0.2)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APRnQBXZ8h5h"
      },
      "source": [
        "## Preprocesamiento (codificación del texto a vectores numéricos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty8fWijE8h5h"
      },
      "source": [
        "# max_length is used for max sequence of input\n",
        "max_length = get_max_length(x_train)\n",
        "\n",
        "train_X, train_Y = preprocess(x_train, y_train, max_length)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3S6lvYe8h5i"
      },
      "source": [
        "## Construcción del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETidgFM48h5i"
      },
      "source": [
        "# LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(CANTIDAD_CLASES, activation='softmax'))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5b-FGyt8h5i"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IcqosK98h5j"
      },
      "source": [
        "### Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nRmzS8w8h5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18552695-7d3a-4a22-f085-5a7cb03a935a"
      },
      "source": [
        "print('Train...')\n",
        "model.fit(train_X, train_Y,epochs=50)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train...\n",
            "Epoch 1/50\n",
            "25/25 [==============================] - 3s 33ms/step - loss: 1.3663 - accuracy: 0.3263\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 1.3000 - accuracy: 0.4153\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 1.2889 - accuracy: 0.4012\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 1.1957 - accuracy: 0.5084\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 1.2104 - accuracy: 0.5149\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 1.0877 - accuracy: 0.5523\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 1.1281 - accuracy: 0.5315\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 1.0408 - accuracy: 0.5799\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.9962 - accuracy: 0.6312\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.9589 - accuracy: 0.6304\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 1.0057 - accuracy: 0.5923\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8749 - accuracy: 0.6832\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8717 - accuracy: 0.6786\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.8584 - accuracy: 0.6695\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.8335 - accuracy: 0.7156\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.8299 - accuracy: 0.6942\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.7907 - accuracy: 0.7036\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.7829 - accuracy: 0.7072\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7275 - accuracy: 0.7197\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7581 - accuracy: 0.7111\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7494 - accuracy: 0.7257\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.6879 - accuracy: 0.7230\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.7114 - accuracy: 0.7515\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.7049 - accuracy: 0.7508\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.7172 - accuracy: 0.7359\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.6477 - accuracy: 0.7647\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.6903 - accuracy: 0.7482\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.6449 - accuracy: 0.7677\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 0.6343 - accuracy: 0.7586\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 1s 35ms/step - loss: 0.6661 - accuracy: 0.7393\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.6634 - accuracy: 0.7367\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.5533 - accuracy: 0.8195\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.6330 - accuracy: 0.7731\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.5362 - accuracy: 0.8073\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.6646 - accuracy: 0.7544\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.5442 - accuracy: 0.7952\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.5135 - accuracy: 0.8152\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.5239 - accuracy: 0.8161\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.5248 - accuracy: 0.8084\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.5199 - accuracy: 0.8155\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.5123 - accuracy: 0.8347\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.5555 - accuracy: 0.7889\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.5576 - accuracy: 0.8126\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.5291 - accuracy: 0.8056\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.4700 - accuracy: 0.8421\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.4762 - accuracy: 0.8268\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.4208 - accuracy: 0.8474\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.4923 - accuracy: 0.8372\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 0.4547 - accuracy: 0.8465\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 1s 32ms/step - loss: 0.4173 - accuracy: 0.8711\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc659cf84d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1z-AzYl8h5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63de52f7-7999-4e0d-a473-4eb27d1332ce"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (32, 32)                  36224     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (32, 4)                   132       \n",
            "=================================================================\n",
            "Total params: 36,356\n",
            "Trainable params: 36,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpAesm--8h5k"
      },
      "source": [
        "### Testeo del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmNeuOIp8h5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d72b1c-1079-42e0-ab90-73a6f7db61d6"
      },
      "source": [
        "# max_length is used for max sequence of input\n",
        "max_length = get_max_length(x_test)\n",
        "\n",
        "test_X, test_Y = preprocess(x_test, y_test, max_length)\n",
        "\n",
        "\n",
        "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 - 1s - loss: 1.0720 - accuracy: 0.6600\n",
            "Test score: 1.0720036029815674\n",
            "Test accuracy: 0.6600000262260437\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}