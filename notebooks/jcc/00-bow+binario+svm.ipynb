{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00-bow+binario+svm.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumafernandez/clasificacion_correos/blob/main/notebooks/jcc/00-bow%2Bbinario%2Bsvm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DuUrVBgeUso"
      },
      "source": [
        "# Baseline JCC: BoW+SVM\n",
        "\n",
        "En esta notebook se presetan los experimentos sobre la estrategia de representación y técnica de aprendizaje *baseline* utilizada para las JCC de  la Universidad Nacional de La Plata.\n",
        "\n",
        "Para ello vamos a preprocesar los correos y aplicar:\n",
        "- Bag of words,\n",
        "- Pesado binario/no binario,\n",
        "- Máquina de vector soporte (SVM).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQvZz035bSMf"
      },
      "source": [
        "# Cargamos el archivo con las consultas que está en Github\n",
        "from os import path\n",
        "\n",
        "# En caso que no esté el archivo en Colab lo traigo\n",
        "if not(path.exists('03-Correos_variables_estaticas.csv')):\n",
        "  !wget https://raw.githubusercontent.com/jumafernandez/clasificacion_correos/main/data/03-Correos_variables_estaticas.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB-MMYgBfpEA"
      },
      "source": [
        "# Leemos el archivo en un dataframe\n",
        "import pandas as pd\n",
        "df = pd.read_csv('03-Correos_variables_estaticas.csv', delimiter=\"|\")\n",
        "\n",
        "# Se transforma proveedor_correo a numerico\n",
        "from sklearn import preprocessing\n",
        "le_correo = preprocessing.LabelEncoder()\n",
        "df['proveedor_correo'] = le_correo.fit_transform(df['proveedor_correo'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg4URhjJvm0y",
        "outputId": "fc89dca1-a271-4138-94e8-e3ad73ea0069"
      },
      "source": [
        "# Me guardo los atributos, excepto la clase en x\r\n",
        "x = df.drop(['Clase'], axis=1)\r\n",
        "x.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Consulta', 'dia_semana', 'semana_del_mes', 'mes', 'cuatrimestre',\n",
              "       'anio', 'hora_discretizada', 'dni_discretizado', 'legajo_discretizado',\n",
              "       'posee_legajo', 'posee_telefono', 'carrera_valor', 'proveedor_correo',\n",
              "       'cantidad_caracteres', 'proporcion_mayusculas', 'proporcion_letras',\n",
              "       'cantidad_tildes', 'cantidad_palabras', 'cantidad_palabras_cortas',\n",
              "       'proporcion_palabras_distintas', 'frecuencia_signos_puntuacion',\n",
              "       'cantidad_oraciones', 'utiliza_codigo_asignatura'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP4lJ_KVvBSO",
        "outputId": "de709b56-bc43-4f66-ef01-b340749d6e51"
      },
      "source": [
        "# Transformamos todas las Clases minoritarias (Puedo ir variando la cantidad de clases que derivo a la Clase \"Otras Consultas\")\r\n",
        "cantidad_clases=4\r\n",
        "\r\n",
        "clases = df.Clase.value_counts()\r\n",
        "clases_minoritarias = clases.iloc[cantidad_clases:].keys().to_list()\r\n",
        "\r\n",
        "df.Clase[df['Clase'].isin(clases_minoritarias)] = \"Otras Consultas\"\r\n",
        "\r\n",
        "# Clases balanceadas\r\n",
        "df.Clase.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Otras Consultas             330\n",
              "Boleto Universitario        240\n",
              "Ingreso a la Universidad    232\n",
              "Requisitos de Ingreso       129\n",
              "Pedido de Certificados       69\n",
              "Name: Clase, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMQlwGgdL_-p"
      },
      "source": [
        "# Se numeriza la clase\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "# Me quedo con las clases numerizadas\n",
        "y=le.fit_transform(df['Clase'])\n",
        "\n",
        "# Por otro lado me guardo las etiquetas de las clases\n",
        "target_names=le.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7IG6gK9Ogeo"
      },
      "source": [
        "# Separo datos de entrenamiento y testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separo en 80-20 entrenamiento/testeo y validación\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17obw6uxVN21"
      },
      "source": [
        "## SVM\n",
        "\n",
        "Generamos el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EfgucE-4KUt",
        "outputId": "db677888-558d-4311-ecc1-bdc55fed8eaa"
      },
      "source": [
        "from sklearn.pipeline import Pipeline, FeatureUnion\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.svm import SVC\r\n",
        "\r\n",
        "# Defino la feature y la transformación a aplicar en el texto\r\n",
        "text_features = 'Consulta'\r\n",
        "text_transformer = CountVectorizer()\r\n",
        "\r\n",
        "# Defino la transformación\r\n",
        "preprocessor = ColumnTransformer(\r\n",
        "    transformers=[\r\n",
        "        ('text', text_transformer, text_features)\r\n",
        "        ])\r\n",
        "\r\n",
        "# Combino la transformación con el pipeline\r\n",
        "model_pipe = Pipeline([('preprocessor', preprocessor),\r\n",
        "                       ('svm', SVC())])\r\n",
        "\r\n",
        "# Defino los parámetros para GridSearchCV\r\n",
        "parameters=[\r\n",
        "        {'preprocessor__text__binary': [True, False],\r\n",
        "         'preprocessor__text__analyzer': ['char'],\r\n",
        "         'preprocessor__text__ngram_range': ((3, 3), (4, 7), (3, 4)),\r\n",
        "         'preprocessor__text__strip_accents': ['unicode'],     \r\n",
        "         'preprocessor__text__max_features':[500, 1000, 1500, 3000],\r\n",
        "         'svm__C': [0.1, 1, 10, 100, 1000],  \r\n",
        "         'svm__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \r\n",
        "         'svm__class_weight': [None, 'balanced'],\r\n",
        "         'svm__kernel': ['rbf', 'linear', 'poly', 'rbf', 'sigmoid']\r\n",
        "    }\r\n",
        "]\r\n",
        "\r\n",
        "# Instancio y \"entreno\" el GridSearchCV\r\n",
        "grid_search=GridSearchCV(model_pipe, param_grid=parameters, cv=None, n_jobs=-1, verbose=3)\r\n",
        "grid_search.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6000 candidates, totalling 30000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   17.6s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed:  4.7min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  7.1min\n",
            "[Parallel(n_jobs=-1)]: Done 1148 tasks      | elapsed:  9.9min\n",
            "[Parallel(n_jobs=-1)]: Done 1564 tasks      | elapsed: 14.9min\n",
            "[Parallel(n_jobs=-1)]: Done 2044 tasks      | elapsed: 21.1min\n",
            "[Parallel(n_jobs=-1)]: Done 2588 tasks      | elapsed: 27.6min\n",
            "[Parallel(n_jobs=-1)]: Done 3196 tasks      | elapsed: 34.1min\n",
            "[Parallel(n_jobs=-1)]: Done 3868 tasks      | elapsed: 40.7min\n",
            "[Parallel(n_jobs=-1)]: Done 4604 tasks      | elapsed: 48.5min\n",
            "[Parallel(n_jobs=-1)]: Done 5404 tasks      | elapsed: 58.6min\n",
            "[Parallel(n_jobs=-1)]: Done 6268 tasks      | elapsed: 71.1min\n",
            "[Parallel(n_jobs=-1)]: Done 7196 tasks      | elapsed: 83.7min\n",
            "[Parallel(n_jobs=-1)]: Done 8188 tasks      | elapsed: 95.1min\n",
            "[Parallel(n_jobs=-1)]: Done 9244 tasks      | elapsed: 109.4min\n",
            "[Parallel(n_jobs=-1)]: Done 10364 tasks      | elapsed: 127.2min\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYcaSAyarqAY"
      },
      "source": [
        "# Imprimo los mejores parámetros encontrados por GridSearchCV\r\n",
        "print(grid_search.best_params_) \r\n",
        "  \r\n",
        "# Imprimo el modelo después del ajuste de hiperparámetros\r\n",
        "print(grid_search.best_estimator_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq3A5ecT19O9"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix \r\n",
        "\r\n",
        "# Se realizan las predicciones sobre el conjunto de validación\r\n",
        "grid_predictions = grid_search.predict(x_test) \r\n",
        "\r\n",
        "# Se imprime el reporte de clasificación\r\n",
        "print(classification_report(y_test, grid_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a1K2447qjxE"
      },
      "source": [
        "# Referencias\r\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\r\n",
        "- https://medium.com/analytics-vidhya/ml-pipelines-using-scikit-learn-and-gridsearchcv-fe605a7f9e05"
      ]
    }
  ]
}