{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06-lr+svm-acc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumafernandez/clasificacion_correos/blob/main/notebooks/jaiio/modelos/06-lr%2Bsvm_acc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DuUrVBgeUso"
      },
      "source": [
        "# Logistic Regression + SVM (dataset acumulativo)\n",
        "\n",
        "En esta notebook se presetan los experimentos a partir de los datos etiquetados automáticamente a partir de las _features_ extraidas del train dataset con la estrategia de LR y SVM.\n",
        "\n",
        "Para ello vamos a preprocesar los correos y aplicar:\n",
        "- Bag of words,\n",
        "- Pesado binario/no binario,\n",
        "- Máquina de vector soporte (SVM).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBLanwDJVnFF"
      },
      "source": [
        "## 1. Instalación y Carga de librerías y funciones útiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-dNi6mDUZnb"
      },
      "source": [
        "### 1.1 Instalación de librerías\n",
        "\n",
        "Se instalan las librerías que no están en el entorno de Google Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "644uDER_VnXj",
        "outputId": "cfb04a73-3910-449c-e97e-f18d13de3b25"
      },
      "source": [
        "# Se instala gensim que es el que tiene el modelo Word2Vec\n",
        "!pip install requests\n",
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43fDuhK2V1kx"
      },
      "source": [
        "### 1.2 Funciones útiles\n",
        "\n",
        "Se cargan funciones útiles desde el repo https://github.com/jumafernandez/clasificacion_correos para la carga y balanceo del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQvZz035bSMf"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Se hace el request del raw del script python\n",
        "url = 'https://raw.githubusercontent.com/jumafernandez/clasificacion_correos/main/scripts/funciones_dataset.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "# Se guarda en el working directory\n",
        "with open('funciones_dataset.py', 'w') as f:\n",
        "    f.write(r.text)\n",
        "\n",
        "# Se importan las funciones a utilizar\n",
        "from funciones_dataset import get_clases, cargar_dataset, consolidar_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDxjaGIfV-rT"
      },
      "source": [
        "También se carga la función para preprocesar el texto que se usó en los otros modelos desde el repo: https://github.com/jumafernandez/clasificacion_correos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0o2NndLWDVC"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Se hace el request del raw del script python\n",
        "url = 'https://raw.githubusercontent.com/jumafernandez/clasificacion_correos/main/scripts/funciones_preprocesamiento.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "# Se guarda en el working directory\n",
        "with open('funciones_preprocesamiento.py', 'w') as f:\n",
        "    f.write(r.text)\n",
        "\n",
        "# Se importan las funciones a utilizar\n",
        "from funciones_preprocesamiento import preprocesar_correos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uv6R5UQWWmE"
      },
      "source": [
        "### 1.3. Carga de datos\n",
        "\n",
        "Se carga el dataframe en memoria con el preprocesamiento de los datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP4lJ_KVvBSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bddb9d5b-ed43-4460-e2a4-100af9150992"
      },
      "source": [
        "import warnings\n",
        "from os import path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Constantes con los datos\n",
        "DS_DIR = 'https://raw.githubusercontent.com/jumafernandez/clasificacion_correos/main/data/50jaiio/consolidados/'\n",
        "TRAIN_FILE = 'correos-train-jaiio-80.csv'\n",
        "TEST_FILE = 'correos-test-jaiio-20.csv'\n",
        "\n",
        "# Chequeo sobre si los archivos están en el working directory\n",
        "download_files = not(path.exists(TRAIN_FILE))\n",
        "\n",
        "etiquetas = get_clases()\n",
        "\n",
        "# Defino la cantidad de clases a utilizar\n",
        "CANTIDAD_CLASES = len(etiquetas)\n",
        "\n",
        "train_df, test_df, etiquetas = cargar_dataset(DS_DIR, TRAIN_FILE, TEST_FILE, download_files, 'clase', etiquetas, CANTIDAD_CLASES, 'Otras Consultas')\n",
        "\n",
        "# Se ejecuta el preprocesamiento de correos sobre el campo Consulta de train y test\n",
        "import pandas as pd\n",
        "train_df['consulta'] = pd.Series(preprocesar_correos(train_df['consulta']))\n",
        "test_df['consulta'] = pd.Series(preprocesar_correos(test_df['consulta']))\n",
        "\n",
        "# Muestro salida por consola\n",
        "print('Existen {} clases: {}.'.format(len(train_df.clase.unique()), train_df.clase.unique()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "El conjunto de entrenamiento tiene la dimensión: (800, 24)\n",
            "El conjunto de testeo tiene la dimensión: (200, 24)\n",
            "Existen 16 clases: ['Boleto Universitario' 'Problemas con la Clave' 'Cursadas'\n",
            " 'Cambio de Carrera' 'Ingreso a la Universidad' 'Requisitos de Ingreso'\n",
            " 'Reincorporación' 'Pedido de Certificados' 'Situación Académica'\n",
            " 'Exámenes' 'Cambio de Comisión' 'Consulta por Legajo'\n",
            " 'Consulta sobre Título Universitario' 'Datos Personales'\n",
            " 'Consulta por Equivalencias' 'Simultaneidad de Carreras'].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IKK2WwlVLye"
      },
      "source": [
        "Me guardo la columna del score de Elasticsearch para no generar incompatibilidades:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2fLeM2_Lspq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e086c01-dc9e-4563-f088-67e94d9e218c"
      },
      "source": [
        "import warnings\n",
        "from os import path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Constantes con los datos\n",
        "DS_DIR = 'https://raw.githubusercontent.com/jumafernandez/clasificacion_correos/main/data/50jaiio/consolidados/'\n",
        "TRAIN_FILE = 'dataset-lr-200-prep.csv'\n",
        "TEST_FILE = 'correos-test-jaiio-20.csv'\n",
        "\n",
        "# Chequeo sobre si los archivos están en el working directory\n",
        "download_files = not(path.exists(TRAIN_FILE))\n",
        "\n",
        "etiquetas = get_clases()\n",
        "\n",
        "# Defino la cantidad de clases a utilizar\n",
        "CANTIDAD_CLASES = len(etiquetas)\n",
        "\n",
        "df_lr, test_df, etiquetas = cargar_dataset(DS_DIR, TRAIN_FILE, TEST_FILE, download_files, 'clase', etiquetas, CANTIDAD_CLASES, 'Otras Consultas')\n",
        "\n",
        "# Se ejecuta el preprocesamiento de correos sobre el campo Consulta de train y test\n",
        "import pandas as pd\n",
        "df_lr['consulta'] = pd.Series(preprocesar_correos(df_lr['consulta']))\n",
        "test_df['consulta'] = pd.Series(preprocesar_correos(test_df['consulta']))\n",
        "\n",
        "df_lr.rename(columns={'clase': 'clase_lr'}, inplace=True)\n",
        "\n",
        "# Muestro salida por consola\n",
        "print('Existen {} clases: {}.'.format(len(df_lr.clase_lr.unique()), df_lr.clase_lr.unique()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "El conjunto de entrenamiento tiene la dimensión: (3200, 25)\n",
            "El conjunto de testeo tiene la dimensión: (200, 24)\n",
            "Existen 16 clases: ['Boleto Universitario' 'Cambio de Carrera' 'Cambio de Comisión'\n",
            " 'Consulta por Equivalencias' 'Consulta por Legajo'\n",
            " 'Consulta sobre Título Universitario' 'Cursadas' 'Datos Personales'\n",
            " 'Exámenes' 'Ingreso a la Universidad' 'Pedido de Certificados'\n",
            " 'Problemas con la Clave' 'Reincorporación' 'Requisitos de Ingreso'\n",
            " 'Simultaneidad de Carreras' 'Situación Académica'].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSsu4FBCMC_R"
      },
      "source": [
        "df_lr = df_lr.query('score>=12').reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK0K2o1TNjpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9aa747-c1ad-4ea1-8aa2-f56373515735"
      },
      "source": [
        "df_lr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(596, 25)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRo77KG9MYd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd004d9-fe54-44ba-bb66-dc3f8b83e3cc"
      },
      "source": [
        "df_lr.rename(columns={'clase_lr': 'clase'}, inplace=True)\n",
        "\n",
        "score = df_lr['score']\n",
        "df_lr.drop('score', inplace=True, axis=1)\n",
        "\n",
        "df_lr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(596, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A1Zma5xMr92"
      },
      "source": [
        "train_df_2 = pd.concat([train_df, df_lr], axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf7kSSKzNY7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a9e29a-ae39-4311-fc4e-dc9fab464f52"
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17obw6uxVN21"
      },
      "source": [
        "## 2. SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOOwBxoYUoWC"
      },
      "source": [
        "### 2.1. Carga de librerías de procesamiento de texto\n",
        "\n",
        "Se cargan en memoria dos funciones: _grid_search_por_estrategia_representacion_ que va a iterar ajustando los hiperparámetros para las técnica de __SVM__ y _representacion_documentos_ que genera representaciones para las _features textuales_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XF6H0BGX9F9"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Se hace el request del raw del script python\n",
        "url = 'https://raw.githubusercontent.com/jumafernandez/clasificacion_correos/main/scripts/funciones_clasificacion_texto.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "# Se guarda en el working directory\n",
        "with open('funciones_clasificacion_texto.py', 'w') as f:\n",
        "    f.write(r.text)\n",
        "\n",
        "# Se importan las funciones a utilizar\n",
        "from funciones_clasificacion_texto import gridsearch_por_estrategia_representacion, representacion_documentos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR6VPw5NNYje"
      },
      "source": [
        "### 2.2 Modelo general (clasificación en las 16 clases)\n",
        "\n",
        "En primer lugar se trabaja con un único clasificador que clasifica y testea las instancias en las 16 clases posibles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kgPWXtpU_7v"
      },
      "source": [
        "#### 2.2.1 Definición del espacio de búsqueda\n",
        "\n",
        "Se define el espacio de búsqueda para el ajuste de hiperparámetros del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3TOc1GcX-rg"
      },
      "source": [
        "# Defino una lista con los esquemas de representación\n",
        "estrategias_representacion = ['BINARIO', 'TFIDF', '3-4-NGRAM-CHARS', '1-2-NGRAM-WORDS']\n",
        "modelo = 'SVM'\n",
        "\n",
        "# Defino los parámetros para GridSearchCV\n",
        "params_svm = {'SVM__C': [0.1, 1, 10, 100], \n",
        "              'SVM__gamma': [0.01, 0.1, 1],\n",
        "              'SVM__class_weight': [None, 'balanced'],\n",
        "              'SVM__kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
        "              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmnm8yPmYGAR"
      },
      "source": [
        "Se ejecuta el ajuste de hiperparámetros para cada estrategia de representación en función del espacio de búsqueda:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFSOquuAYGPK"
      },
      "source": [
        "NO_CORRIDA = False\n",
        "\n",
        "if NO_CORRIDA:\n",
        "  for estrategia in estrategias_representacion:\n",
        "    # Llamo a la función que realiza el gridsearch por estrategia  \n",
        "    gridsearch_por_estrategia_representacion(train_df, test_df, estrategia, modelo, params_svm, None, atr_consulta='consulta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjoUvaOgh00W"
      },
      "source": [
        "#### 2.2.2 Modelo generado\n",
        "\n",
        "En función de los mejores hiperparámetros encontrados con la búsqueda Grid, ajusto el modelo para obtener las métricas (por un bug en la función ya solucionado):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfU5JT1vh0J3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95f514c-1735-4f89-b655-4d3f9cff05c6"
      },
      "source": [
        "# Mejores Parámetros según GridSearchCV:\n",
        "# Parámetros: {'SVM__C': 100, 'SVM__class_weight': None, 'SVM__gamma': 0.01, 'SVM__kernel': 'sigmoid', \n",
        "# 'clasificador': 'SVM', 'estrategia': '3-4-NGRAM-CHARS', 'accuracy': 0.58, 'precision': 0.5180902430902431, \n",
        "# 'recall': 0.5621418525934401, 'f1_score': 0.49018479841316037}\n",
        "\n",
        "params_svm = {'SVM__C': [100], \n",
        "              'SVM__gamma': [0.01],\n",
        "              'SVM__class_weight': [None],\n",
        "              'SVM__kernel': ['sigmoid'],\n",
        "              'SVM__probability': [True]\n",
        "              }\n",
        "\n",
        "clf, X_test, y_test = gridsearch_por_estrategia_representacion(train_df, test_df, '3-4-NGRAM-CHARS', 'SVM', params_svm, None, atr_consulta='consulta')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estrategia de representación: 3-4-NGRAM-CHARS\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   40.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Estrategia de representación: 3-4-NGRAM-CHARS\n",
            "Parámetros: {'SVM__C': 100, 'SVM__class_weight': None, 'SVM__gamma': 0.01, 'SVM__kernel': 'sigmoid', 'SVM__probability': False, 'clasificador': 'SVM', 'estrategia': '3-4-NGRAM-CHARS', 'accuracy': 0.795, 'precision': 0.5862921138282899, 'recall': 0.5395589006483601, 'f1_score': 0.5527751039214867}\n",
            "Accuracy Test-Set: 0.795\n",
            "Métricas sobre Test-Set: {'SVM__C': 100, 'SVM__class_weight': None, 'SVM__gamma': 0.01, 'SVM__kernel': 'sigmoid', 'SVM__probability': False, 'clasificador': 'SVM', 'estrategia': '3-4-NGRAM-CHARS', 'accuracy': 0.795, 'precision': 0.5862921138282899, 'recall': 0.5395589006483601, 'f1_score': 0.5527751039214867}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzVZFzCTVTSZ"
      },
      "source": [
        "Se predicen las instancias de testeo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNoj8Gshc09R"
      },
      "source": [
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoOcZO15Vni2"
      },
      "source": [
        "Además, se utiliza el método _predict\\_proba_ para calcular la probabilidad asociada a la clasificación de cada instancia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVQ-oYM0Pwof",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "e496058c-a5da-4b02-9602-cfa651e1e1e8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "y_pred_proba = np.around(clf.predict_proba(X_test), 2)\n",
        "\n",
        "y_max_proba = np.amax(y_pred_proba, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-322d4660d821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_max_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[1;32m    604\u001b[0m                                  \" probability=False\")\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'c_svc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nu_svc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsR7UjZZUzTy"
      },
      "source": [
        "y_max_proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgSm2WQzVyUt"
      },
      "source": [
        "Se genera un _dataframe_ con las clases observadas para _test_, las predicciones y las probabilidades asociadas para verificar si hay relación entre las proba y los errores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsF3kAQjS5By"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "resultados = pd.concat([pd.Series(y_test), pd.Series(y_pred), pd.Series(y_max_proba)], axis=1)\n",
        "resultados.columns = ['clase', 'prediccion', 'max_proba']\n",
        "resultados['correcto'] = resultados['clase'] == resultados['prediccion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dvHDGdSV_1S"
      },
      "source": [
        "Se calcula el _accuracy__ para las predicciones con una proba>0.5 y se obtiene una mejora sustancial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1-NV0ztUmlq"
      },
      "source": [
        "sum(resultados[resultados.max_proba>0.5]['correcto'])/resultados[resultados.max_proba>0.5]['correcto'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIpzEIaOWKNa"
      },
      "source": [
        "#### 2.2.3 Métricas de selección"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cied9MM_WNvG"
      },
      "source": [
        "##### 2.2.3.1. Matriz de confusión\n",
        "\n",
        "A continuación, se calcula la matriz de confusión para cada clase para el recorte con proba>0.5 y para el modelo con la totalidad de instancias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeK6rWwqZInr"
      },
      "source": [
        "from sklearn import metrics #Importar el módulo metrics de scikit-learn\n",
        "\n",
        "# Vamos a testear el modelo\n",
        "print(\"Accuracy:\", metrics.accuracy_score(resultados[resultados.max_proba>0.5]['clase'], resultados[resultados.max_proba>0.5]['prediccion']))\n",
        "\n",
        "# Vemos un reporte de clasificación de varias métricas\n",
        "print(metrics.classification_report(resultados[resultados.max_proba>0.5]['clase'], resultados[resultados.max_proba>0.5]['prediccion']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-THNpdciSP"
      },
      "source": [
        "from sklearn import metrics #Importar el módulo metrics de scikit-learn\n",
        "\n",
        "# Vamos a testear el modelo\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Vemos un reporte de clasificación de varias métricas\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8dTwLl5dYu-"
      },
      "source": [
        "metrics.confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYUVKbHldjKq"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns; sns.set()\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mat = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=etiquetas, yticklabels=etiquetas)\n",
        "\n",
        "plt.xlabel('Observada')\n",
        "plt.ylabel('Predicha');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P14cduVB6pZk"
      },
      "source": [
        "### 3. Análisis del error\n",
        "\n",
        "A continuación se intenta entender la baja de _accuracy_ para el etiquetado a partir de TF-IDF.\n",
        "\n",
        "Para ello, se genera el dataset de train con: <br/>\n",
        "| consulta | clase | score |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yzf0qf36oL2"
      },
      "source": [
        "df_train_e = train_df[['consulta', 'clase']]\n",
        "df_train_e['score'] = score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJVVj9Kk6uhd"
      },
      "source": [
        "Genero un dataframe con los scores promedios por clase, el accuracy por clase y el _count_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRdVnXSN6wkJ"
      },
      "source": [
        "df_error = df_train_e.groupby(['clase']).mean().reset_index()\n",
        "\n",
        "# Accuracy por clase\n",
        "avg_class = mat.diagonal()/mat.sum(axis=1)\n",
        "df_error['accuracy'] = pd.Series(avg_class)\n",
        "\n",
        "df_error['count'] = mat.sum(axis=1)\n",
        "\n",
        "df_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRKtSNPr63Oo"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"ticks\")\n",
        "\n",
        "sns.scatterplot(data=df_error, x=\"score\", y=\"accuracy\", hue=\"count\", size=\"count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a1K2447qjxE"
      },
      "source": [
        "## Referencias\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "- https://medium.com/analytics-vidhya/ml-pipelines-using-scikit-learn-and-gridsearchcv-fe605a7f9e05"
      ]
    }
  ]
}